{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPO1nRC7vEggF9ZsgO+obH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Machine_Learning/blob/main/RandomForestClassificationFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def entropy(y):\n",
        "    \"\"\"\n",
        "    Compute entropy of a label array y.\n",
        "    y: array-like of shape (n_samples,)\n",
        "    \"\"\"\n",
        "    # Count occurrences of each class\n",
        "    counts = np.bincount(y)\n",
        "    probabilities = counts[counts > 0] / len(y)\n",
        "\n",
        "    # H = -sum p * log2(p)\n",
        "    return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "\n",
        "def information_gain(y, y_left, y_right):\n",
        "    \"\"\"\n",
        "    Compute information gain of a split:\n",
        "    IG = H(parent) - (n_L/n * H(left) + n_R/n * H(right))\n",
        "    \"\"\"\n",
        "    H_parent = entropy(y)\n",
        "\n",
        "    n = len(y)\n",
        "    n_left = len(y_left)\n",
        "    n_right = len(y_right)\n",
        "\n",
        "    if n_left == 0 or n_right == 0:\n",
        "        # No actual split\n",
        "        return 0\n",
        "\n",
        "    H_left = entropy(y_left)\n",
        "    H_right = entropy(y_right)\n",
        "\n",
        "    weighted_child_entropy = (n_left / n) * H_left + (n_right / n) * H_right\n",
        "\n",
        "    return H_parent - weighted_child_entropy\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self,\n",
        "                 feature_index=None,\n",
        "                 threshold=None,\n",
        "                 left=None,\n",
        "                 right=None,\n",
        "                 value=None):\n",
        "        \"\"\"\n",
        "        If value is not None, this is a leaf node.\n",
        "        Otherwise, it is a decision node, with feature_index and threshold.\n",
        "        \"\"\"\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value  # class label for leaf\n",
        "\n",
        "class DecisionTreeClassifierScratch:\n",
        "    def __init__(self,\n",
        "                 max_depth=None,\n",
        "                 min_samples_split=2,\n",
        "                 min_impurity_decrease=1e-7):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Build the decision tree from training data.\n",
        "        X: numpy array of shape (n_samples, n_features)\n",
        "        y: numpy array of shape (n_samples,)\n",
        "        \"\"\"\n",
        "        self.n_classes_ = len(np.unique(y))\n",
        "        self.n_features_ = X.shape[1]\n",
        "        # ipdb.set_trace(context=10)\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        Recursively build the decision tree.\n",
        "        \"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        num_labels = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions:\n",
        "        # 1. Pure node (all labels the same)\n",
        "        # 2. Not enough samples to split\n",
        "        # 3. Reached max depth\n",
        "        if (num_labels == 1 or\n",
        "            num_samples < self.min_samples_split or\n",
        "            (self.max_depth is not None and depth >= self.max_depth)):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            # ipdb.set_trace(context = 10)\n",
        "            return TreeNode(value=leaf_value)\n",
        "\n",
        "        # Find the best split: feature and threshold\n",
        "        best_feature, best_threshold, best_gain = self._best_split(X, y)\n",
        "\n",
        "        # If no useful split (gain too small), create leaf\n",
        "        if best_gain < self.min_impurity_decrease:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return TreeNode(value=leaf_value)\n",
        "\n",
        "        # Split\n",
        "        left_indices = X[:, best_feature] <= best_threshold\n",
        "        right_indices = X[:, best_feature] > best_threshold\n",
        "\n",
        "        X_left, y_left = X[left_indices], y[left_indices]\n",
        "        X_right, y_right = X[right_indices], y[right_indices]\n",
        "\n",
        "        # Recursively build children\n",
        "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        # Return decision node\n",
        "        return TreeNode(feature_index=best_feature,\n",
        "                        threshold=best_threshold,\n",
        "                        left=left_child,\n",
        "                        right=right_child)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Try all features and candidate thresholds and return the best split.\n",
        "        \"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        best_gain = -1\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        for feature_index in range(num_features):\n",
        "            # Consider unique values of this feature as candidate thresholds\n",
        "            feature_values = X[:, feature_index]\n",
        "            unique_values = np.unique(feature_values)\n",
        "\n",
        "            # Optionally, we could consider midpoints between sorted unique values.\n",
        "            # For simplicity, let's use unique_values directly as thresholds.\n",
        "            for threshold in unique_values:\n",
        "                left_indices = feature_values <= threshold\n",
        "                right_indices = feature_values > threshold\n",
        "\n",
        "                y_left = y[left_indices]\n",
        "                y_right = y[right_indices]\n",
        "\n",
        "                gain = information_gain(y, y_left, y_right)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    # ipdb.set_trace(context=10)\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        \"\"\"\n",
        "        Return the most frequent class label in y.\n",
        "        \"\"\"\n",
        "        counter = Counter(y)\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "        \"\"\"\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"\n",
        "        Traverse the tree from the root to a leaf for sample x.\n",
        "        \"\"\"\n",
        "        # If leaf\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        # Go left or right depending on threshold\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(x, node.right)\n",
        "\n",
        "\n",
        "class RandomForestClassifierScratch:\n",
        "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_features=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.n_features = n_features # Number of features to consider at each split\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTreeClassifierScratch(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split\n",
        "            )\n",
        "            # 1. Bootstrapping: Sample indices with replacement\n",
        "            n_samples = X.shape[0]\n",
        "            idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "            X_sample, y_sample = X[idxs], y[idxs]\n",
        "\n",
        "            # Note: For true Feature Randomness, you would modify the _best_split\n",
        "            # in the DecisionTree class to only iterate over a random subset of indices.\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Gather predictions from every tree\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        # tree_preds shape: (n_trees, n_samples). Transpose to (n_samples, n_trees)\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "\n",
        "        # 2. Majority Voting\n",
        "        y_pred = [Counter(sample_preds).most_common(1)[0][0] for sample_preds in tree_preds]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "X, y = make_moons(n_samples=300, noise=0.3, random_state=42)\n",
        "\n",
        "# 2. Train the Random Forest\n",
        "clf = RandomForestClassifierScratch(n_trees=10, max_depth=5)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# 3. Create a mesh grid for plotting\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Predict over the mesh grid\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# 4. Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, edgecolor='k', cmap='RdBu')\n",
        "plt.title(\"Random Forest Classification Boundary (Scratch)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YGsvjZaTXEHQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}